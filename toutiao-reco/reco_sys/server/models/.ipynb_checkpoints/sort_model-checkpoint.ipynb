{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# 如果当前代码文件运行测试需要加入修改路径，避免出现后导包问题\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "sys.path.insert(0, os.path.join(BASE_DIR))\n",
    "\n",
    "PYSPARK_PYTHON = \"/miniconda2/envs/reco_sys/bin/python\"\n",
    "# 当存在多个版本时，不指定很可能会导致出错\n",
    "os.environ[\"PYSPARK_PYTHON\"] = PYSPARK_PYTHON\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = PYSPARK_PYTHON\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from offline import SparkSessionBase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiSortModel(SparkSessionBase):\n",
    "\n",
    "    SPARK_APP_NAME = \"SortModel\"\n",
    "    ENABLE_HIVE_SUPPORT = True\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.spark = self._create_spark_hbase()\n",
    "        \n",
    "ms = MultiSortModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1、进行数据处理与以及用户画像读取\n",
    "ms.spark.sql(\"use profile\")\n",
    "user_article_basic = ms.spark.sql(\"select user_id, article_id, channel_id, clicked from user_article_basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_profile_hbase = ms.spark.sql(\n",
    "    \"select user_id, information.birthday, information.gender, article_partial, env from user_profile_hbase\")\n",
    "# 删除无用的一些用户基础信息\n",
    "user_profile_hbase = user_profile_hbase.drop('env')\n",
    "\n",
    "# 增加RDD类型指定\n",
    "_schema = StructType([\n",
    "    StructField('user_id', LongType()),\n",
    "    StructField('birthday', DoubleType()),\n",
    "    StructField('gender', BooleanType()),\n",
    "    StructField('article_partial', MapType(StringType(), DoubleType()))\n",
    "])\n",
    "def get_user_id(row):\n",
    "    return int(row.user_id.split(\":\")[1]), row.birthday, row.gender, row.article_partial\n",
    "\n",
    "# user_profile_hbase_temp = user_profile_hbase.rdd.map(get_user_id).toDF(['user_id', 'birthday', 'gender', 'article_partial'])\n",
    "user_profile_hbase_temp = user_profile_hbase.rdd.map(get_user_id)\n",
    "user_profile_hbase_schema = ms.spark.createDataFrame(user_profile_hbase_temp, schema=_schema)\n",
    "user_profile_article = user_article_basic.join(user_profile_hbase_schema, on=['user_id'], how='left').drop('channel_id')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms.spark.sql(\"use article\")\n",
    "article_vector = ms.spark.sql(\"select * from article_vector\")\n",
    "train = user_profile_article.join(article_vector, on=['article_id'], how='left').drop('birthday').drop('gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------+--------------------+----------+--------------------+\n",
      "|article_id|            user_id|clicked|     article_partial|channel_id|       articlevector|\n",
      "+----------+-------------------+-------+--------------------+----------+--------------------+\n",
      "|     13401|1114864237131333632|  false|Map(18:vars -> 0....|        18|[0.06157120217893...|\n",
      "|     13401|                 10|  false|Map(18:tp2 -> 0.2...|        18|[0.06157120217893...|\n",
      "|     13401|1106396183141548032|  false|Map(18:tp2 -> 0.2...|        18|[0.06157120217893...|\n",
      "|     13401|1109994594201763840|  false|Map(18:tp2 -> 0.2...|        18|[0.06157120217893...|\n",
      "|     14805|1106473203766657024|  false|Map(18:text -> 0....|        18|[0.11028526511434...|\n",
      "|     14805|1113049054452908032|  false|Map(18:text -> 0....|        18|[0.11028526511434...|\n",
      "|     14805|1114863751909081088|   true|Map(18:text -> 2....|        18|[0.11028526511434...|\n",
      "|     14805|1115534909935452160|  false|Map(18:text -> 2....|        18|[0.11028526511434...|\n",
      "|     14805|1103195673450250240|  false|Map(18:Animal -> ...|        18|[0.11028526511434...|\n",
      "|     14805|1105045287866466304|  false|Map(18:text -> 0....|        18|[0.11028526511434...|\n",
      "|     14805|1114864237131333632|  false|Map(18:vars -> 0....|        18|[0.11028526511434...|\n",
      "|     14805|1109995264376045568|  false|Map(18:text -> 0....|        18|[0.11028526511434...|\n",
      "|     14805|1111524501104885760|  false|Map(18:text -> 0....|        18|[0.11028526511434...|\n",
      "|     14805|1105105185656537088|  false|Map(18:SHOldboySt...|        18|[0.11028526511434...|\n",
      "|     14805|1114864128259784704|  false|Map(18:text -> 0....|        18|[0.11028526511434...|\n",
      "|     14805|1114864233264185344|  false|Map(18:text -> 0....|        18|[0.11028526511434...|\n",
      "|     14805|1115436666438287360|  false|Map(18:text -> 0....|        18|[0.11028526511434...|\n",
      "|     14805|1114863846486441984|  false|Map(18:vars -> 0....|        18|[0.11028526511434...|\n",
      "|     14805|1115089292662669312|  false|Map(18:vars -> 0....|        18|[0.11028526511434...|\n",
      "|     14805|1114863902073552896|  false|Map(18:Animal -> ...|        18|[0.11028526511434...|\n",
      "+----------+-------------------+-------+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['user_id', 'article_id', 'channel_id', 'weights', 'clicked']\n",
    "\n",
    "def get_user_vector(row):\n",
    "    \"\"\"得到用户和文章的特征向量\n",
    "    \"\"\"\n",
    "    from pyspark.ml.linalg import Vectors\n",
    "    \n",
    "    try:\n",
    "        weights = sorted([row.article_partial[key] for key in row.article_partial.keys() \n",
    "                          if int(key.split(\":\")[0]) == row.channel_id])[:10]\n",
    "    except Exception as e:\n",
    "        weights = [0.0] * 10\n",
    "    \n",
    "    return row.user_id, row.article_id, row.channel_id, Vectors.dense(weights), int(row.clicked)\n",
    "    \n",
    "train = train.rdd.map(get_user_vector).toDF(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+--------------------+-------+\n",
      "|            user_id|article_id|channel_id|             weights|clicked|\n",
      "+-------------------+----------+----------+--------------------+-------+\n",
      "|1114864237131333632|     13401|        18|[0.32473420471378...|      0|\n",
      "|                 10|     13401|        18|[0.21215332784742...|      0|\n",
      "|1106396183141548032|     13401|        18|[0.22553064631951...|      0|\n",
      "|1109994594201763840|     13401|        18|[0.24443647588626...|      0|\n",
      "|1106473203766657024|     14805|        18|[0.22553064631951...|      0|\n",
      "|1113049054452908032|     14805|        18|[0.28050889359956...|      0|\n",
      "|1114863751909081088|     14805|        18|[0.32473420471378...|      1|\n",
      "|1115534909935452160|     14805|        18|[0.35819704778381...|      0|\n",
      "|1103195673450250240|     14805|        18|[0.21442838668808...|      0|\n",
      "|1105045287866466304|     14805|        18|[0.21952219380422...|      0|\n",
      "|1114864237131333632|     14805|        18|[0.32473420471378...|      0|\n",
      "|1109995264376045568|     14805|        18|[0.24443647588626...|      0|\n",
      "|1111524501104885760|     14805|        18|[0.26087773109487...|      0|\n",
      "|1105105185656537088|     14805|        18|[0.21952219380422...|      0|\n",
      "|1114864128259784704|     14805|        18|[0.32473420471378...|      0|\n",
      "|1114864233264185344|     14805|        18|[0.32473420471378...|      0|\n",
      "|1115436666438287360|     14805|        18|[0.35819704778381...|      0|\n",
      "|1114863846486441984|     14805|        18|[0.32473420471378...|      0|\n",
      "|1115089292662669312|     14805|        18|[0.33945366606672...|      0|\n",
      "|1114863902073552896|     14805|        18|[0.32473420471378...|      0|\n",
      "+-------------------+----------+----------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 用户特征处理结束\n",
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文章的特征中心结果\n",
    "ms.spark.sql(\"use profile\")\n",
    "ctr_feature_article_hbase = ms.spark.sql(\"select * from ctr_feature_article_hbase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|article_id|     article_feature|\n",
      "+----------+--------------------+\n",
      "|         1|Map(1 -> [17.0,0....|\n",
      "|        10|Map(10 -> [17.0,0...|\n",
      "|       100|Map(100 -> [17.0,...|\n",
      "|     10001|Map(10001 -> [19....|\n",
      "|    100010|Map(100010 -> [17...|\n",
      "|    100011|Map(100011 -> [17...|\n",
      "|    100012|Map(100012 -> [17...|\n",
      "|    100013|Map(100013 -> [17...|\n",
      "|    100014|Map(100014 -> [17...|\n",
      "|    100015|Map(100015 -> [17...|\n",
      "|    100016|Map(100016 -> [17...|\n",
      "|    100017|Map(100017 -> [17...|\n",
      "|    100018|Map(100018 -> [17...|\n",
      "|    100019|Map(100019 -> [17...|\n",
      "|     10002|Map(10002 -> [19....|\n",
      "|    100020|Map(100020 -> [17...|\n",
      "|    100021|Map(100021 -> [17...|\n",
      "|     10003|Map(10003 -> [19....|\n",
      "|     10004|Map(10004 -> [15....|\n",
      "|    100042|Map(100042 -> [17...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ctr_feature_article_hbase.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.join(ctr_feature_article_hbase, on=['article_id'], how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----------+--------------------+-------+--------------------+\n",
      "|article_id|            user_id|channel_id|             weights|clicked|     article_feature|\n",
      "+----------+-------------------+----------+--------------------+-------+--------------------+\n",
      "|     13401|1114864237131333632|        18|[0.32473420471378...|      0|Map(13401 -> [18....|\n",
      "|     13401|                 10|        18|[0.21215332784742...|      0|Map(13401 -> [18....|\n",
      "|     13401|1106396183141548032|        18|[0.22553064631951...|      0|Map(13401 -> [18....|\n",
      "|     13401|1109994594201763840|        18|[0.24443647588626...|      0|Map(13401 -> [18....|\n",
      "|     14805|1106473203766657024|        18|[0.22553064631951...|      0|Map(14805 -> [18....|\n",
      "|     14805|1113049054452908032|        18|[0.28050889359956...|      0|Map(14805 -> [18....|\n",
      "|     14805|1114863751909081088|        18|[0.32473420471378...|      1|Map(14805 -> [18....|\n",
      "|     14805|1115534909935452160|        18|[0.35819704778381...|      0|Map(14805 -> [18....|\n",
      "|     14805|1103195673450250240|        18|[0.21442838668808...|      0|Map(14805 -> [18....|\n",
      "|     14805|1105045287866466304|        18|[0.21952219380422...|      0|Map(14805 -> [18....|\n",
      "|     14805|1114864237131333632|        18|[0.32473420471378...|      0|Map(14805 -> [18....|\n",
      "|     14805|1109995264376045568|        18|[0.24443647588626...|      0|Map(14805 -> [18....|\n",
      "|     14805|1111524501104885760|        18|[0.26087773109487...|      0|Map(14805 -> [18....|\n",
      "|     14805|1105105185656537088|        18|[0.21952219380422...|      0|Map(14805 -> [18....|\n",
      "|     14805|1114864128259784704|        18|[0.32473420471378...|      0|Map(14805 -> [18....|\n",
      "|     14805|1114864233264185344|        18|[0.32473420471378...|      0|Map(14805 -> [18....|\n",
      "|     14805|1115436666438287360|        18|[0.35819704778381...|      0|Map(14805 -> [18....|\n",
      "|     14805|1114863846486441984|        18|[0.32473420471378...|      0|Map(14805 -> [18....|\n",
      "|     14805|1115089292662669312|        18|[0.33945366606672...|      0|Map(14805 -> [18....|\n",
      "|     14805|1114863902073552896|        18|[0.32473420471378...|      0|Map(14805 -> [18....|\n",
      "+----------+-------------------+----------+--------------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_col = ['user_id', 'article_id', 'clicked', 'weights', 'article_feature']\n",
    "def get_artilce_vector(row):\n",
    "    \"\"\"得到用户和文章的特征向量\n",
    "    \"\"\"\n",
    "    from pyspark.ml.linalg import Vectors\n",
    "    \n",
    "    try:\n",
    "        article_feature = row.article_feature[str(row.article_id)]\n",
    "    except Exception as e:\n",
    "        article_feature = [0.0] * 111\n",
    "    \n",
    "    return row.user_id, row.article_id, row.clicked, row.weights, Vectors.dense(eval(article_feature))\n",
    "    \n",
    "train_vector = train.rdd.map(get_artilce_vector).toDF(train_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 做特征的指定指定合并\n",
    "train_version_two = VectorAssembler().setInputCols(train_col[3:5]).setOutputCol(\"features\").transform(train_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_version_two.collect()[0].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存到TFRecords文件中\n",
    "df = train_version_two.select(['user_id', 'article_id', 'clicked', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[user_id: bigint, article_id: bigint, clicked: bigint, features: vector]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.write.format(\"tfrecords\").option(\"recordType\", \"SequenceExample\").save(\"hdfs://hadoop-master:9000/headlines/models/train_2019_04.tfrecord\")、\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_array = df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3616"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(df_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.2122, 0.2122, 0.2122, 0.2122, 0.2122, 0.2122, 0.2122, 0.2122, 0.2122, 0.2122, 18.0, 0.082, 0.1122, 0.1354, 0.1609, 0.1636, 0.1674, 0.1809, 0.1907, 0.2025, 0.21, 0.0616, 0.0357, -0.0008, 0.0916, 0.0128, 0.0312, 0.01, 0.0486, -0.0301, -0.0107, -0.0806, 0.0339, -0.0161, 0.0753, -0.0265, 0.0253, 0.0032, 0.0101, -0.0164, -0.0068, -0.0297, 0.0114, -0.0295, 0.0204, -0.0644, -0.0579, 0.0539, 0.0694, 0.0305, -0.0371, -0.0005, 0.0513, 0.0726, 0.076, -0.062, 0.0006, -0.0688, -0.056, 0.0494, -0.0069, 0.0606, -0.0675, -0.0136, 0.0348, 0.0012, 0.0384, 0.1002, 0.0362, -0.0677, 0.0049, -0.0127, -0.0424, 0.0532, 0.0469, 0.0091, 0.0149, 0.0103, -0.0039, -0.0102, 0.0628, -0.0004, -0.043, -0.0063, -0.0909, 0.0228, 0.0317, -0.0361, -0.0195, 0.0156, -0.0577, -0.0216, -0.0115, -0.0083, -0.006, 0.0198, 0.0407, 0.0341, 0.0037, 0.0411, -0.012, 0.0607, -0.0582, 0.0332, -0.0119, 0.0353, 0.0342, 0.0203, -0.0416, -0.0406, 0.0761, 0.0172, 0.0546, 0.0476, 0.0052, -0.0009, -0.0017, -0.0463, -0.0645, -0.0216, 0.1021])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, 3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.train.start_queue_runners()` was called when no queue runners were defined. You can safely remove the call to this deprecated function.\n"
     ]
    }
   ],
   "source": [
    "def write_to_tfrecords(click_batch, feature_batch):\n",
    "        \"\"\"\n",
    "        将数据存进tfrecords，方便管理每个样本的属性\n",
    "        :param image_batch: 特征值\n",
    "        :param label_batch: 目标值\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # 1、构造tfrecords的存储实例\n",
    "        writer = tf.python_io.TFRecordWriter(\"./train_ctr_201904.tfrecords\")\n",
    "        # 2、循环将每个样本写入到文件当中\n",
    "        for i in range(len(click_batch)):\n",
    "\n",
    "            click = click_batch[i]\n",
    "            feature = feature_batch[i].tostring()\n",
    "\n",
    "            # 绑定每个样本的属性\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[click])),\n",
    "                \"feature\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[feature])),\n",
    "            }))\n",
    "            writer.write(example.SerializeToString())\n",
    "\n",
    "        # 文件需要关闭\n",
    "        writer.close()\n",
    "        return None\n",
    "\n",
    "# 开启会话打印内容\n",
    "with tf.Session() as sess:\n",
    "    # 创建线程协调器\n",
    "    coord = tf.train.Coordinator()\n",
    "\n",
    "    # 开启子线程去读取数据\n",
    "    # 返回子线程实例\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    # 存入数据\n",
    "    write_to_tfrecords(df.iloc[:, 2], df.iloc[:, 3])\n",
    "\n",
    "    # 关闭子线程，回收\n",
    "    coord.request_stop()\n",
    "\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
